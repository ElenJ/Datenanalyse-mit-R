---
title: "Regression"
output: html_document
date: "2023-01-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Regression


Die Regression gibt einen Zusammenhang zwischen zwei oder mehr Variablen an, wobei die Zielvariable numerisch ist. Bei der Regressionsanalyse wird vorausgesetzt, dass es einen gerichteten linearen Zusammenhang gibt, das heißt, es existieren eine abhängige Variable und mindestens eine unabhängige Variable. Ob es einen ZUsammenhang zwischen der Zielvariablen und ihrer Prädiktoren gibt, können wir durch verschiede Verfahren versuchen zu ergründen. 

Wir laden zunächst einige hilfreiche Bibliotheken:
```{r}
library(tidyverse) # für Tabellenbearbeitung und ggplot2
library(ggpubr) # hilfreiche Erweiterung der Plots
library(GGally) #plottet Parameter paarweise + errechnet Grundstatistiken
```

## 1. Korrelation

Was bedeutet Korrelation?

Sehr vereinfacht, heißt es, dass der Verlauf einer Variablen A dem Verlauf einer Variable B ähnelt, ohne dass es einen Zusammenhang geben muss. **„Correlation does not imply causation"** Um sich das zu vergegenwärtigen, sollte man eins der vielen Beispiele im Kopf haben, die [hier](https://www.tylervigen.com/spurious-correlations) gesammelt wurden.

Die Stärke der Korrelation wird durch den Pearson‘schen Korrelationskoeffizienten r oder das Bestimmtheitsmaß R=r^2 angegeben. Dabei sagt das Bestimmtheitsmaß, welchen Anteil der Veränderung eines Parameters B sich aus den Veränderungen des Parameters A erklären lässt.

Die Korrelation zwischen zwei numerischen Variablen lässt sich in R leicht berechnen. Als Beispiel betrachten wir den iris-Datensatz, mit dem schon Fisher experimentiert hat. Dabei handelt es sich um die Aufzeichnung von Blüten- und Kelchblattlängen/breiten von 3 verschiedenen Irisarten.
```{r}
head(iris)
```

Man könnte vermuten, dass es eine Abhängigkeit zwischen Sepalenlänge und -breite geben müsste. Wir schauen es uns zunächst im Plot an und finden eine interessante Eigenschaft:
```{r}
a <- ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + 
  geom_point() + 
  stat_cor(method = "pearson", label.x = c(6), label.y = 4) + 
  geom_smooth(method = lm, se = F) + #zeichnet die Linie
  ggtitle("Kelchblattbreite vs. Kelchblattlänge")

b <- ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) + 
  geom_point() + 
  stat_cor(method = "pearson", label.x = 6, label.y = c(4,4.5,5)) + 
  geom_smooth(method = lm, se = F) + #zeichnet die Linie
  ggtitle("Kelchblattbreite vs. Kelchblattlänge \nverschiedener Iris Arten") + 
  theme(legend.position="bottom")

figure <- ggarrange(a,b,
                    labels = c("A", "B"),
                    ncol = 2)
figure
```
Hier sehen wir ein Beispiel für Simpsons Paradox. Über alle Arten betrachtet, scheint die Kelchblattlänge mit -breite negativ assoziiert zu sein (die Korrelation ist nicht signifikant, daher kann man hier eigentlich nicht sagen, dass es eine Korrelation gibt). Berechnet man die Korrelation jedoch innerhalb der einzelnen Arten, so sieht man innerhalb jeder Art eine starke positive Korrelation, d.h. innerhalb einer Irisart korreliert die Sepalenlänge sehr wohl mit ihrer Breite. Unsere Vermutung scheint bestätigt zu werden. 

Wir können auch gleich alle Parameter des Datensatzes gegeneinander zeichnen und vergleichen:
```{r}
ggpairs(iris, aes(colour = Species, alpha = 0.4))
```
Offenbar gibt es auch einen linearen Zusammenhang zwischen der Petalenweite und -Breite. 

Wir können auch die Korrelationen zwischen den einzelnen Parametern in einer Heatmap plotten. Das geht mit ggplot():
```{r}
#errechnen der Korrelation und Formatieren des Datensatzes
df_cor <- iris %>% select(-Species) %>% cor() %>% as.data.frame()
df_cor$Var1 <- rownames(df_cor)
df_long <- gather(df_cor, Var2, value, -Var1)
#Heatmap
ggplot(data = df_long, aes(x=Var1, y=Var2, fill=value)) + 
   geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))
```

Alternativ können wir auf eine weitere schöne Bibliothek für Korrelationsdiagramme zurückgreifen: corrplot.
```{r}
library(corrplot)
#Farbdefinition
col<- colorRampPalette(c("blue", "white", "red"))(20)
#Plot
corrplot(iris %>% select(-Species) %>% cor(), method="circle", type="upper", col=col)
```
### Korrelationsbedingungen
Strenggenommen waren wir mit der Betrachtung der Korrelation zwischen den einzelnen Parametern etwas voreilig. Der errechnete p-Wert hat nämlich nur Belang, wenn die ff. Bedingungen erfüllt sind:  

1. Ist der Zusammenhang zwischen den zu untersuchenden Parametern linear?

2. Sind die Daten normalverteilt?

Wenn diese Voraussetzungen nicht erfüllt sind, so sind auch die ermittelten p-Werte ohne jeglichen Aussagewert. 

Die Linearitär einer Beziehung zwischen zwei numerischen Variablen lässt sich am einfachsten einfach durch die visuelle Inspektion ermitteln. Folgen die Daten keiner Geraden, sondern z.B. eher einer sigmoiden oder exponentiellen Kurve, kann man den Korrelationstest nicht durchführen.   

Die Normalverteilung lässt sich visuell mittels des sog. QQ-Plots abschätzen oder formal mit dem Shapiro-Wilk Test prüfen. Da wir im iris-Datensatz bereits gesehen haben, dass die Betrachtung pro Art erfolgen sollte, müssen wir beachten, dass wir die Prüfungen für jede Art einzeln machen.
```{r}
for (irisspecies in unique(iris$Species)){
  print(irisspecies)
  #Filtern des Datensatzes pro Art
  df_subset <- iris %>% filter(Species==irisspecies)
  #Shapiro-Wilk Test
  print(shapiro.test(df_subset$Sepal.Length))
  
  #der QQ-Plot
  print(ggplot(df_subset, aes(sample = Sepal.Length)) +
   stat_qq() + stat_qq_line()+
    ggtitle(paste0("Sepal.Length von ",irisspecies," - QQPlot"))) 
}
```

Um mal ein Gegenbeispiel zu demonstrieren, zeichnen wir den QQ-Plot für eine Parabel:
```{r}
# generiere Daten
x <- -100:100
y <- x^2
df <- data.frame(x = x, y = y)
# der Plot
ggplot(df, aes(sample = y)) +
  stat_qq() + 
  stat_qq_line()+
  ggtitle("QQ Plot einer Parabel") +
  theme_minimal()

# der Test
shapiro.test(df$y)
```
Im Plot erkennt man, dass die beiden Enden von der Garaden abweichen, was auf eine nicht-Normalverteilung hindeutet. DAs sieht man auch am niedrigen p-Wert des Shapiro-Wilk Tests.

**Aufgabe:** Prüfen Sie, ob die Bedingungen für eine Korrelation auch für Sepal.Width greifen.

Sofern dies zutrifft, dürfen wir den Korrelationstest durchführen.

### Korrelationstest
Zunächst führen wir den Test auf dem gesamten Datensatz durch, um die Funktion kennenzulernen:
```{r}
corr_result <- cor.test(iris$Sepal.Length, iris$Sepal.Width, method = "pearson")
corr_result
```
Es interessieren uns hauptsächlich die Werte cor (der Korrelationskoeffizient, der zugehörige p-Wert sowie das Konfidenzintervall). Wir können diese Parameter auch folgendermaßen isolieren:
```{r}
corr_result$estimate
corr_result$p.value
corr_result$conf.int
```

Jetzt analysieren wir die Korrelation für jede Art und schreiben die Ergebnisse in eine Tabelle:
```{r}
corr_result_table <- data.frame()
for (irisspecies in unique(iris$Species)){
  print(irisspecies)
  #Filtern des Datensatzes pro Art
  iris_subset <- iris %>% filter(Species==irisspecies)
  #Korrelationstest
  corr_result <- cor.test(iris_subset$Sepal.Length, iris_subset$Sepal.Width, method = "pearson")
  #Schreiben der Ergebnisse in eine Tabelle
  single_result <- c(irisspecies,signif(corr_result$estimate,3),signif(corr_result$p.value,3),signif(corr_result$conf.int[1],3),signif(corr_result$conf.int[2],3))
  corr_result_table <- rbind(corr_result_table,single_result)
}
#Vergeben sinnvoller Spaltennamen
colnames(corr_result_table) <- c("Species","Corr.Koeff","p.value","ConfInt_low","ConfInt_high")
corr_result_table
```

Der p-Wert lässt uns die Nullhypothese verwerfen (dass es keine Korrelation gibt) und wir können annehmen, dass innerhalb jeder der untersuchten Irisarten die Kelchblattlänge und -breite positiv korreliert sind. 

Natürlich sollten Sie die Heatmaps der Korrelation ebenfalls pro Irisart zeichnen.

## 2. Regression: Lineare Modellierung

Es gibt verschiedene Arten der Regression, je nachdem, welche Art von Daten man vorhersagt:

- Lineare Regression: Y ist eine numerische Variable

- Logistische Regression: Y ist nominal

- Cox Regression: Y ist Zeit bis zum Eintreffen eines Events

- Poisson Regression: Y ist eine Abzählung

## Lineare Regression

Eine lineare Regression nimmt an, dass es einen linearen Zusammenhang zwiwschen den Parametern X und der Zielvariablen Y gibt. Dabei gilt es, die Koeffizienten zu ermitteln, die durch die Gleichung der linearen Regression die Zielvariable im Trainingsdatenssatz mit dem geringsten Fehler vorhersagen. Der Unterschied zur Korrelation lässt sich folgendermaßen verdeutlichen: bei der Korrelation verändern sich Parameter A und B im vergleichbarem Maße (z.B. linker, rechter Zeigefinger), es gibt aber nicht unbedingt eine (einseitige) Abhängigkeit der Parameter voneinander. Bei der Regression hingegen liegt eine einseitige Abhängigkeit der Form X --> Y vor (Körpergröße Eltern --> Körpergröße Kind). 

Die lineare Regression hat die Form:

\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\end{equation}

Es gilt, solche β‘s zu finden, dass der Fehler der Vorhersage von Y bezüglich tatsächlicher Y minimiert wird (min RSS). Außerdem wird geprüft, ob β sich signifikant von 0 unterscheidet. Dabei kann X auch ein Polynom des 2en, 3n, x-ten Grades sein, es bleibt immer noch eine lineare Regression.

## Antworten der linearen Regression

1. Gibt es eine Abhängigkeit zwischen Y und X1, X2 etc.? Ist β signifikant von 0 verschieden?

Dazu betrachtet man die F-Statistik, t-Statistik und die dazugehörigen p-Werte.

2. Wie stark ist diese Abhängigkeit?

Dies sagt uns die Größe und Genauigkeit (SE) von β 

3. Wie genau können wir Y vorhersagen?

Dazu betrachten wir den Residual Standard Error RSE

4. Welchen Teil der Variabilität von Y können wir vorhersagen?

Verbirgt sich hinter R^2^.

5. Gibt es Synergien zwischen X-en?

Dazu analysiert man die Interaktionen zwischen Xen.

## Iris Beispiel
Wir haben bereits gesehen, dass es eine Korrelation zwischen der Petalenweite und -länge innerhalb des Irisdatensatzes gab. Mittels der linearen Regression könnten wir ein Modell bauen, dass uns z.B. die Weite vorhersagt, wenn wir die Länge kennen.

Versuchen wir eine einfache lineare Regression.
```{r}
lin_reg_petal <- lm(Petal.Width ~ Petal.Length, data = iris)
summary(lin_reg_petal)
```

Sowohl die F- als auch t-Statistik zeigen eine Abhängigkeit zwischen Petal.Length und Petal.Width (kleiner p-Wert). Jedoch machen wir noch ein paar Tests:
```{r}
par(mfrow = c(2, 2))
plot(lin_reg_petal)
```
Residuals vs Fitted: wir sehen eine m.o.w. zufällige Verteilung der Punkte. Der QQ Plot zeigt, dass die Verteilung der Residuen annähernd einer Normalverteilung folgt.  Der Scale-Location Plot zeigt, dass wir es mit Heteroskedastizität (=Varianzheterogenität) zu tun haben (die rote Linie ist nicht gerade)(-->, roblem zu beheben). Aus Cook's Plot sehen wir, dass die Punkte 108, 123 und 135 einen starken Einfluss auf das Modell haben. Dies ist eine weitere Möglichkeit, nach Ausreißern zu schauen. rstudent Werte >3 sind Kandidaten für Ausreißer.

```{r}
plot(predict(lin_reg_petal), rstudent(lin_reg_petal))
```
## Multivariate Regression

Wir weiten die Anzahl der Parameter unseres Modells aus:
```{r}
lm_fit_komplett <-  lm(Petal.Width ~ . - Species, data = iris)
summary(lm_fit_komplett)
```

Was erkennen sie?

Gibt es einen Zusammenhang zwischen anderen Parametern und Petal.Width? Was bedeutet der Schätzer für Sepal.Length konkret?
